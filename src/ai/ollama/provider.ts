import { ChatCompletionTool } from "openai/resources";
import { Chat, getTextContent } from "../../messages/chat.js";
import { AxleMessage, ContentPartToolCall } from "../../messages/types.js";
import { Recorder } from "../../recorder/recorder.js";
import { ToolDef } from "../../tools/types.js";
import { AIProvider, AIRequest, AxleStopReason, GenerationResult } from "../types.js";
import { OllamaRequest, OllamaSystemMessage } from "./types.js";
import { convertAxleMessagesToOllama, convertToolDefToOllama } from "./utils.js";

const DEFAULT_OLLAMA_URL = "http://localhost:11434";

export class OllamaProvider implements AIProvider {
  name = "Ollama";
  url: string;
  model: string;
  recorder?: Recorder;

  constructor(model: string, url?: string) {
    this.url = url || DEFAULT_OLLAMA_URL;
    this.model = model;
  }

  createChatRequest(chat: Chat, context: { recorder?: Recorder } = {}): AIRequest {
    const { recorder } = context;
    if (chat.hasFiles()) {
      recorder?.warn?.log(
        `Ollama model ${this.model} multimodal support depends on the specific model. Ensure you're using a vision-capable model like llava.`,
      );
    }
    return new OllamaChatCompletionRequest(this.url, this.model, chat);
  }

  async createGenerationRequest(params: {
    messages: Array<AxleMessage>;
    tools?: Array<ToolDef>;
    context: { recorder?: Recorder };
  }): Promise<GenerationResult> {
    return await createGenerationRequest({
      url: this.url,
      model: this.model,
      ...params,
    });
  }
}

async function createGenerationRequest(params: {
  url: string;
  model: string;
  messages: Array<AxleMessage>;
  tools?: Array<ToolDef>;
  context: { recorder?: Recorder };
}): Promise<GenerationResult> {
  const { url, model, messages, tools, context } = params;
  const { recorder } = context;

  const chatTools = convertToolDefToOllama(tools);

  const requestBody = {
    model,
    messages: convertAxleMessagesToOllama(messages),
    stream: false,
    options: {
      temperature: 0.7,
    },
    ...(chatTools && { tools: chatTools }),
  };

  recorder?.debug?.log(requestBody);

  let result: GenerationResult;
  try {
    const response = await fetch(`${url}/api/chat`, {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
      },
      body: JSON.stringify(requestBody),
    });

    if (!response.ok) {
      throw new Error(`HTTP error! status: ${response.status}`);
    }

    const data = await response.json();
    result = fromModelResponse(data);
  } catch (e) {
    recorder?.error?.log("Error fetching Ollama response:", e);
    result = {
      type: "error",
      error: {
        type: "OllamaError",
        message: e.message || "Unexpected error from Ollama",
      },
      usage: {
        in: 0,
        out: 0,
      },
      raw: JSON.stringify(e),
    };
  }

  recorder?.debug?.log(result);
  return result;
}

class OllamaChatCompletionRequest implements AIRequest {
  chat: Chat;
  url: string;
  model: string;

  constructor(url: string, model: string, chat: Chat) {
    this.url = url;
    this.model = model;
    this.chat = chat;
  }

  async execute(runtime: { recorder?: Recorder }): Promise<GenerationResult> {
    const { recorder } = runtime;
    const requestBody = {
      stream: false,
      options: {
        temperature: 0.7,
      },
      ...prepareRequest(this.chat, this.model),
    };

    recorder?.debug?.log(requestBody);

    let result: GenerationResult;
    try {
      const response = await fetch(`${this.url}/api/chat`, {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
        },
        body: JSON.stringify(requestBody),
      });

      if (!response.ok) {
        console.log(response);
        throw new Error(`HTTP error! status: ${response.status}`);
      }

      const data = await response.json();
      result = fromModelResponse(data);
    } catch (e) {
      recorder?.error?.log("Error fetching Ollama response:", e);
      result = {
        type: "error",
        error: {
          type: "OllamaError",
          message: e.message || "Unexpected error from Ollama",
        },
        usage: {
          in: 0,
          out: 0,
        },
        raw: JSON.stringify(e),
      };
    }

    recorder?.debug?.log(result);
    return result;
  }
}

/**
 * This is similar to the OpenAI format, except the tool call arguments
 * needs to match that generated by the model, which can be an object.
 * @param chat
 * @returns
 */
export function prepareRequest(chat: Chat, model: string): OllamaRequest {
  const systemMsg = prepareSystemMessage(chat);
  const tools = prepareTools(chat);
  const messages = convertAxleMessagesToOllama(chat.messages);

  return {
    model,
    messages: [...systemMsg, ...messages],
    ...(tools && { tools }),
  };
}

function prepareSystemMessage(chat: Chat): OllamaSystemMessage[] {
  if (!chat.system) {
    return [];
  }

  return [
    {
      role: "system",
      content: chat.system,
    },
  ];
}

function prepareTools(chat: Chat): ChatCompletionTool[] | undefined {
  if (chat.tools.length === 0) {
    return undefined;
  }

  return chat.tools.map((schema) => ({
    type: "function",
    function: schema,
  }));
}

function fromModelResponse(data: any): GenerationResult {
  if (data.done_reason === "stop" && data.message) {
    const content = data.message.content;
    const toolCalls: ContentPartToolCall[] = [];
    if (data.message.tool_calls) {
      for (const call of data.message.tool_calls) {
        toolCalls.push({
          type: "tool-call",
          id: call.id,
          name: call.function.name,
          arguments: call.function.arguments,
        });
      }
    }
    const hasToolCalls = toolCalls.length > 0;
    const contentParts = [{ type: "text" as const, text: content }];

    return {
      type: "success",
      id: `ollama-${Date.now()}`,
      model: data.model,
      role: "assistant",
      reason: hasToolCalls ? AxleStopReason.FunctionCall : AxleStopReason.Stop,
      content: contentParts,
      text: getTextContent(contentParts) ?? "",
      ...(hasToolCalls && { toolCalls }),
      usage: {
        in: data.prompt_eval_count || 0,
        out: data.eval_count || 0,
      },
      raw: data,
    };
  }

  return {
    type: "error",
    error: {
      type: "OllamaError",
      message: "Unexpected error from Ollama",
    },
    usage: {
      in: 0,
      out: 0,
    },
    raw: data,
  };
}
